<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="Introducing the basics of PyTorch Tensor. Tensor是表示多维数组的一种数据结构。它类似于NumPy ndarray。">
<meta property="og:type" content="article">
<meta property="og:title" content="The ABCs of PyTorch Tensor">
<meta property="og:url" content="http://yoursite.com/2019/07/30/The-ABCs-of-PyTorch-Tensor/index.html">
<meta property="og:site_name" content="MARS">
<meta property="og:description" content="Introducing the basics of PyTorch Tensor. Tensor是表示多维数组的一种数据结构。它类似于NumPy ndarray。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2019-09-02T11:07:11.889Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="The ABCs of PyTorch Tensor">
<meta name="twitter:description" content="Introducing the basics of PyTorch Tensor. Tensor是表示多维数组的一种数据结构。它类似于NumPy ndarray。">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/07/30/The-ABCs-of-PyTorch-Tensor/">





  <title>The ABCs of PyTorch Tensor | MARS</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">MARS</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br>
            
            日程表
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/30/The-ABCs-of-PyTorch-Tensor/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MARS">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/messi.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MARS">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">The ABCs of PyTorch Tensor</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-07-30T16:01:52+08:00">
                2019-07-30
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/PyTorch/" itemprop="url" rel="index">
                    <span itemprop="name">PyTorch</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/07/30/The-ABCs-of-PyTorch-Tensor/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count gitment-comments-count" data-xid="/2019/07/30/The-ABCs-of-PyTorch-Tensor/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>Introducing the basics of PyTorch Tensor. Tensor是表示多维数组的一种数据结构。它类似于NumPy <code>ndarray</code>。</p>
<a id="more"></a>
<p>Written by <strong>Mars</strong> at 2019/7/30 17:30</p>
<h1 id="Tensors-attributes"><a href="#Tensors-attributes" class="headerlink" title="Tensors attributes"></a>Tensors attributes</h1><p>Tensor是表示多维数组的一种数据结构。它类似于NumPy<code>ndarray</code>。 它的<code>size</code>相当于NumPy <code>ndarray</code>的<code>shape</code>。</p>
<p>Tensor attributes中有三个类，分别为<code>torch.dtype</code>，<code>torch.device</code>和<code>torch.layout</code>。</p>
<p>下面是一个示例程序。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize</span></span><br><span class="line">x = torch.Tensor(<span class="number">2</span>, <span class="number">3</span>) </span><br><span class="line">y = torch.rand(<span class="number">2</span>, <span class="number">3</span>)    </span><br><span class="line"></span><br><span class="line"><span class="comment"># Operations</span></span><br><span class="line">z1 = x + y</span><br><span class="line">z2 = torch.add(x, y)</span><br></pre></td></tr></table></figure>
<h2 id="Torch-dtype"><a href="#Torch-dtype" class="headerlink" title="Torch.dtype"></a>Torch.dtype</h2><p><code>torch.dtype</code> 属性标识了 <code>torch.Tensor</code>的数据类型。PyTorch 有8种不同的数据类型：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Data type</th>
<th style="text-align:center">dtype</th>
<th style="text-align:center">Tensor types</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">32-bit floating point</td>
<td style="text-align:center"><code>torch.float32</code> or <code>torch.float</code></td>
<td style="text-align:center"><code>torch.*.FloatTensor</code></td>
</tr>
<tr>
<td style="text-align:center">64-bit floating point</td>
<td style="text-align:center"><code>torch.float64</code> or <code>torch.double</code></td>
<td style="text-align:center"><code>torch.*.DoubleTensor</code></td>
</tr>
<tr>
<td style="text-align:center">16-bit floating point</td>
<td style="text-align:center"><code>torch.float16</code> or <code>torch.half</code></td>
<td style="text-align:center"><code>torch.*.HalfTensor</code></td>
</tr>
<tr>
<td style="text-align:center">8-bit integer (unsigned)</td>
<td style="text-align:center"><code>torch.uint8</code></td>
<td style="text-align:center"><code>torch.*.ByteTensor</code></td>
</tr>
<tr>
<td style="text-align:center">8-bit integer (signed)</td>
<td style="text-align:center"><code>torch.int8</code></td>
<td style="text-align:center"><code>torch.*.CharTensor</code></td>
</tr>
<tr>
<td style="text-align:center">16-bit integer (signed)</td>
<td style="text-align:center"><code>torch.int16</code> or <code>torch.short</code></td>
<td style="text-align:center"><code>torch.*.ShortTensor</code></td>
</tr>
<tr>
<td style="text-align:center">32-bit integer (signed)</td>
<td style="text-align:center"><code>torch.int32</code> or <code>torch.int</code></td>
<td style="text-align:center"><code>torch.*.IntTensor</code></td>
</tr>
<tr>
<td style="text-align:center">64-bit integer (signed)</td>
<td style="text-align:center"><code>torch.int64</code> or <code>torch.long</code></td>
<td style="text-align:center"><code>torch.*.LongTensor</code></td>
</tr>
</tbody>
</table>
</div>
<h2 id="Torch-device"><a href="#Torch-device" class="headerlink" title="Torch.device"></a>Torch.device</h2><p><code>torch.device</code> 属性标识了<code>torch.Tensor</code>对象在创建之后所存储在的设备名称，而在对象创建之前此属性标识了即将为此对象申请存储空间的设备名称。</p>
<p><code>torch.device</code> 包含了两种设备类型 (<code>&#39;cpu&#39;</code> 或者 <code>&#39;cuda&#39;</code>) ，分别标识将Tensor对象储存于CPU内存或者GPU内存中，同时支持指定设备编号，比如多张GPU，可以通过GPU编号指定某一块GPU。 如果没有指定设备编号，则默认将对象存储于<code>current_device()</code>当前设备中。举个例子， 一个<code>torch.Tensor</code>对象构造函数中的设备字段如果填写<code>&#39;cuda&#39;</code>，那等价于填写了<code>&#39;cuda:X&#39;</code>，其中X是函数 <code>torch.cuda.current_device()</code>的返回值。</p>
<p>在<code>torch.Tensor</code>对象创建之后，可以通过访问<code>Tensor.device</code>属性实时访问当前对象所存储在的设备名称。</p>
<p><code>torch.device</code> 对象支持使用字符串或者字符串加设备编号这两种方式来创建：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### 通过字符串创建</span></span><br><span class="line"></span><br><span class="line">torch.device(<span class="string">"cuda:0"</span>)  <span class="comment"># 编号为0的cuda(gpu)设备</span></span><br><span class="line"><span class="comment"># device(type='cuda', index=0)</span></span><br><span class="line"></span><br><span class="line">torch.device(<span class="string">"cpu"</span>)  <span class="comment"># cpu内存</span></span><br><span class="line"><span class="comment"># device(type='cpu')</span></span><br><span class="line"></span><br><span class="line">torch.device(<span class="string">"cuda"</span>)  <span class="comment"># 当前的cuda(gpu)设备</span></span><br><span class="line"><span class="comment"># device(type='cuda')</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### 通过字符串加设备编号创建</span></span><br><span class="line"></span><br><span class="line">torch.device(<span class="string">"cuda"</span>, <span class="number">0</span>)  </span><br><span class="line"><span class="comment"># device(type='cuda', index=0)</span></span><br><span class="line"></span><br><span class="line">torch.device(<span class="string">"cpu"</span>, <span class="number">0</span>)  </span><br><span class="line"><span class="comment"># device(type='cpu', index=0)</span></span><br></pre></td></tr></table></figure>
<p>此外，cpu 和 cuda 设备的转换使用 ‘to’ 来实现:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">device_cpu = torch.device(<span class="string">"cuda"</span>)  <span class="comment"># 声明cuda设备</span></span><br><span class="line">device_gpu = torch.device(<span class="string">"cpu"</span>)  <span class="comment"># 声明cpu设备</span></span><br><span class="line">data = torch.tensor([<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">data.to(device_cpu)  <span class="comment"># 将数据转为cpu</span></span><br><span class="line">data.to(device_gpu)  <span class="comment"># 将数据转为gpu</span></span><br></pre></td></tr></table></figure>
<p><strong>Note:</strong></p>
<ol>
<li>当<code>torch.device</code>作为函数的参数的时候，可以直接用字符串替换。这样有助于加快代码创建原型的速度。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一个接受torch.device对象为参数的函数例子</span></span><br><span class="line">cuda0 = torch.device(<span class="string">"cuda:0"</span>)</span><br><span class="line">torch,randn((<span class="number">2</span>, <span class="number">3</span>), device=cuda0)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以用一个字符串替换掉torch.device对象，一样的效果</span></span><br><span class="line">torch.randn((<span class="number">2</span>, <span class="number">3</span>), <span class="string">"cuda:0"</span>)</span><br></pre></td></tr></table></figure>
<ol>
<li>由于一些历史遗留问题，device对象还可以仅通过一个设备编号来创建，这些设备编号对应的都是相应的cuda设备。这正好对应了 <code>Tensor.get_device()</code>函数, 这个仅支持cuda Tensor的函数返回的就是当前tensor所在的cuda设备编号，cpu Tensor不支持这个函数。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.device(<span class="number">0</span>)  <span class="comment"># device(type='cuda', index=1)</span></span><br></pre></td></tr></table></figure>
<ol>
<li>接受device参数的函数同时也可以接受一个正确格式的字符串或者正确代表设备编号的数字（数字这个是历史遗留问题）作为参数，以下的操作是等价的：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.randn((<span class="number">2</span>,<span class="number">3</span>), device=torch.device(<span class="string">"cuda:0"</span>))</span><br><span class="line">torch.randn((<span class="number">2</span>,<span class="number">3</span>), device=<span class="string">"cuda:0"</span>)</span><br><span class="line">torch.randn((<span class="number">2</span>,<span class="number">3</span>), device=<span class="number">0</span>)  <span class="comment"># 历史遗留做法</span></span><br></pre></td></tr></table></figure>
<h2 id="Torch-layout"><a href="#Torch-layout" class="headerlink" title="Torch.layout"></a>Torch.layout</h2><p><code>torch.layout</code>属性标识了<code>torch.Tensor</code>在内存中的布局模式。现在支持了两种内存布局模式 <code>torch.strided</code> (dense Tensors) 和尚处试验阶段的<code>torch.sparse_coo</code>(sparse COO Tensors，一种经典的稀疏矩阵存储方式)。</p>
<p><code>torch.strided</code> 跨步存储代表了密集张量的存储布局方式，当然也是最常用最经典的一种布局方式。 每一个strided tensor都有一个与之相连的<code>torch.Storage</code>对象，这个对象存储着tensor的数据。这些Storage对象为tensor提供了一种多维的，跨步的(strided)数据视图。这一视图中的strides是一个interger整形列表：这个列表的主要作用是给出当前张量的各个维度的所占内存大小，严格的定义就是，strides中的第k个元素代表了在第k维度下，从一个元素跳转到下一个元素所需要跨越的内存大小。 <strong>跨步</strong>这个概念有助于提高多种张量运算的效率。</p>
<p>For Example：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.Tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>]])</span><br><span class="line">x.stride()  <span class="comment">#  (5, 1) 此时在这个二维张量中，在第0维度下，从一个元素到下一个元素需要跨越的内存大小是5;</span></span><br><span class="line">            <span class="comment"># 比如x[0] 到x[1]需要跨越x[0]这5个元素, 在第1维度下，是1，如x[0, 0]到x[0, 1]需要跨越1个元素</span></span><br><span class="line">    </span><br><span class="line">x.t().stride()  <span class="comment"># (1, 5)</span></span><br></pre></td></tr></table></figure>
<h1 id="Create-a-Tensor"><a href="#Create-a-Tensor" class="headerlink" title="Create a Tensor"></a>Create a Tensor</h1><h2 id="直接创建"><a href="#直接创建" class="headerlink" title="直接创建"></a>直接创建</h2><p><code>torch.tensor(data, dtype=None, device=None,requires_grad=False)</code></p>
<ul>
<li>data - 可以是list, tuple, numpy array, scalar或其他类型</li>
<li>dtype - 可以返回想要的tensor类型</li>
<li>device - 可以指定返回的设备</li>
<li>requires_grad - 可以指定是否进行记录图的操作，默认为False</li>
</ul>
<p>需要注意的是，torch.tensor 总是会复制 data，如果你想避免复制，可以使用<code>torch.Tensor.detach()</code>，如果是从 numpy 中获得数据，那么你可以用 <code>torch.from_numpy()</code>, 注:<code>from_numpy()</code>是共享内存的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了结果的可重现性，通常首先将随机种子设置为特定值</span></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"----------------------Some test---------------------------"</span>)</span><br><span class="line">print(<span class="string">"torch.cuda.is_available()   ="</span>, torch.cuda.is_available())</span><br><span class="line">print(<span class="string">"torch.cuda.device_count()   ="</span>, torch.cuda.device_count())</span><br><span class="line">print(<span class="string">"torch.cuda.device('cuda')   ="</span>, torch.cuda.device(<span class="string">"cuda"</span>))</span><br><span class="line">print(<span class="string">"torch.cuda.current_device() ="</span>, torch.cuda.current_device())</span><br><span class="line">print(<span class="string">"----------------------Some test---------------------------"</span>)</span><br><span class="line"></span><br><span class="line">torch.tensor([[<span class="number">0.1</span>, <span class="number">1.2</span>], [<span class="number">2.2</span>, <span class="number">3.1</span>], [<span class="number">4.9</span>, <span class="number">5.2</span>]])</span><br><span class="line"></span><br><span class="line">torch.tensor([<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">torch.tensor([[<span class="number">0.11111</span>, <span class="number">0.222222</span>, <span class="number">0.3333333</span>]], </span><br><span class="line">             device = torch.device(<span class="string">"cuda:0"</span>),</span><br><span class="line">             dtype=torch.float64)</span><br></pre></td></tr></table></figure>
<h2 id="从Numpy-创建"><a href="#从Numpy-创建" class="headerlink" title="从Numpy 创建"></a>从Numpy 创建</h2><p><code>torch.from_numpy(ndarry)</code></p>
<p>Note:Tensor与ndarray转换期间，ndarray和Tensor共享相同的内存。任何一方值的改变都会影响另一方。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Numpy --&gt; Tensor</span></span><br><span class="line">a = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">v = torch.from_numpy(a)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Tensor --&gt; Numpy</span></span><br><span class="line">b = v.numpy()                   </span><br><span class="line"></span><br><span class="line">b[<span class="number">1</span>] = <span class="number">-1</span>            <span class="comment"># 共享内存</span></span><br><span class="line"><span class="keyword">assert</span>(a[<span class="number">1</span>] == b[<span class="number">1</span>]) <span class="comment"># Change Numpy will also change the Tensor</span></span><br></pre></td></tr></table></figure>
<h2 id="创建特定的Tensor"><a href="#创建特定的Tensor" class="headerlink" title="创建特定的Tensor"></a>创建特定的Tensor</h2><h3 id="根据数值要求"><a href="#根据数值要求" class="headerlink" title="根据数值要求"></a>根据数值要求</h3><ol>
<li><p><code>torch.zeros(*sizes, out=None, ..)</code> : 返回大小为sizes的零矩阵 </p>
</li>
<li><p><code>torch.zeros_like(input, ..)</code> : 返回与input相同size的零矩阵</p>
</li>
<li><p><code>torch.ones(*sizes, out=None, ..)</code> : 返回大小为sizes的单位矩阵</p>
</li>
<li><p><code>torch.ones_like(input, ..)</code> : 返回与input相同size的单位矩阵</p>
</li>
<li><p><code>torch.full(size, fill_value, …)</code> : 返回大小为sizes,单位值为fill_value的矩阵</p>
</li>
<li><p><code>torch.full_like(input, fill_value, …)</code>: 返回与input相同size，单位值为fill_value的矩阵</p>
</li>
<li><p><code>torch.arange(start=0, end, step=1, …)</code> : 返回从start到end, 单位步长为step的1-d tensor.</p>
</li>
<li><p><code>torch.linspace(start, end, steps=100, …)</code> : 返回从start到end, 间隔中的插值数目为steps的1-d tensor</p>
</li>
<li><p><code>torch.logspace(start, end, steps=100, …)</code> : 返回1-d tensor ，从10\^start到10\^end的steps个</p>
</li>
</ol>
<h3 id="根据矩阵要求"><a href="#根据矩阵要求" class="headerlink" title="根据矩阵要求"></a>根据矩阵要求</h3><ol>
<li><p><code>torch.eye(n, m=None, out=None,…)</code> : 返回2-D 的单位对角矩阵</p>
</li>
<li><p><code>torch.empty(*sizes, out=None, …)</code> : 返回被未初始化的数值填充，大小为sizes的tensor</p>
</li>
<li><p><code>torch.empty_like(input, …)</code> : 返回与input相同size,并被未初始化的数值填充的tensor</p>
</li>
</ol>
<h2 id="随机采样生成"><a href="#随机采样生成" class="headerlink" title="随机采样生成"></a>随机采样生成</h2><ol>
<li><p><code>torch.normal(mean, std, out=None)</code> : 返回一个张量，包含从给定参数means,std的离散正态分布中抽取随机数</p>
</li>
<li><p><code>torch.rand(*size, out=None, dtype=None, …)</code> : 返回[0, 1)之间均匀分布的随机数值</p>
</li>
<li><p><code>torch.rand_like(input, dtype=None, …)</code> : 返回与input相同size的tensor, 填充均匀分布的随机数值</p>
</li>
<li><p><code>torch.randint(low=0, high, size,…)</code> : 返回均匀分布的[low, high]之间的整数随机值</p>
</li>
<li><p><code>torch.randint_like(input, low=0, high, dtype=None, …)</code> : 返回与input相同size的均匀分布的[low, high]之间的整数随机值</p>
</li>
<li><p><code>torch.randn(*sizes, out=None, …)</code> : 返回大小为size,由均值为0，方差为1的正态分布的随机数值</p>
</li>
<li><p><code>torch.randn_like(input, dtype=None, …)</code> : 返回与input相同size的大小为size,由均值为0，方差为1的正态分布的随机数值</p>
</li>
<li><p><code>torch.randperm(n, out=None, dtype=torch.int64)</code> : 返回0到n-1的数列的随机排列</p>
</li>
</ol>
<h1 id="Tensor-Operations"><a href="#Tensor-Operations" class="headerlink" title="Tensor Operations"></a>Tensor Operations</h1><h2 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h2><h3 id="索引-amp-掩码"><a href="#索引-amp-掩码" class="headerlink" title="索引 &amp; 掩码"></a>索引 &amp; 掩码</h3><ol>
<li><p><code>torch.index_select(input, dim, index, out=None)</code> : 返回沿着dim的指定tensor, index需为longTensor类型，不共用内存</p>
</li>
<li><p><code>torch.masked_select(input, mask, out=None)</code> : 根据mask来返回input的值其为1-D tensor. Mask为ByteTensor, true返回，false不返回，返回值不共用内存</p>
</li>
<li><p><code>torch.gather(input, dim, index, out=None)</code> : 返回沿着dim收集的新的tensor</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">9</span>)</span><br><span class="line">x = x.view(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 0 1 2</span></span><br><span class="line"><span class="comment"># 3 4 5</span></span><br><span class="line"><span class="comment"># 6 7 8</span></span><br><span class="line"></span><br><span class="line">indices = torch.LongTensor([<span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line">torch.index_select(x, <span class="number">1</span>, indices)  <span class="comment"># dim=1 横向 选取0、2列</span></span><br><span class="line"></span><br><span class="line">mask = x.ge(<span class="number">5</span>)  <span class="comment"># 掩码前5个</span></span><br><span class="line">torch.masked_select(x, mask)</span><br></pre></td></tr></table></figure>
<p><code>gather</code>可在one-hot作为输出的多分类问题中，把最大值的索引输进去以获得预测值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Gather element</span></span><br><span class="line"><span class="comment"># torch.gather(input, dim, index, out=None)</span></span><br><span class="line"><span class="comment"># out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0</span></span><br><span class="line"><span class="comment"># out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1</span></span><br><span class="line"><span class="comment"># out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2</span></span><br><span class="line"></span><br><span class="line">torch.gather(x, <span class="number">1</span>, torch.LongTensor([[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">0</span>],[<span class="number">2</span>,<span class="number">1</span>]]))</span><br><span class="line"><span class="comment"># 0  1</span></span><br><span class="line"><span class="comment"># 4  3</span></span><br><span class="line"><span class="comment"># 8  7</span></span><br></pre></td></tr></table></figure>
<h3 id="连接-amp-堆栈"><a href="#连接-amp-堆栈" class="headerlink" title="连接 &amp; 堆栈"></a>连接 &amp; 堆栈</h3><p>Note: <code>cat</code> 和 <code>stack</code>的区别在于<code>cat</code>会增加现有维度的值,可以理解为续接，<code>stack</code>会新加增加一个维度，可以理解为叠加</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">torch.cat((x, x, x), <span class="number">0</span>)  <span class="comment"># 纵向拼接，即垂直方向</span></span><br><span class="line"></span><br><span class="line">x.size()  <span class="comment"># Size: 3x3</span></span><br><span class="line"></span><br><span class="line">v = torch.stack((x, x))  <span class="comment"># 增加维度 叠加</span></span><br><span class="line">v.size()  <span class="comment"># Size: 2x3x3</span></span><br></pre></td></tr></table></figure>
<h3 id="分割-amp-分块"><a href="#分割-amp-分块" class="headerlink" title="分割 &amp; 分块"></a>分割 &amp; 分块</h3><ol>
<li><p><code>torch.split(tensor, split_size_or_sections, dim=0)</code> : 将tensor 拆分成相应的组块</p>
</li>
<li><p><code>torch.chunk(tensor, chunks, dim=0)</code> : 将tensor 拆分成相应的组块， 最后一块会小一些如果不能整除的话</p>
</li>
</ol>
<p><code>split</code>和<code>chunk</code>的区别在于：split的split_size_or_sections 表示每一个组块中的数据大小，chunks表示组块的数量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">r = torch.split(x, <span class="number">2</span>)  <span class="comment"># 不能整除时，返回的tuple第一个元素块数较大</span></span><br><span class="line"></span><br><span class="line">r = torch.chunk(x, <span class="number">3</span>)  <span class="comment"># 将 3x3 的 x 分割成含有三个 1x3 的元组</span></span><br></pre></td></tr></table></figure>
<h3 id="Mutation-ops"><a href="#Mutation-ops" class="headerlink" title="Mutation ops"></a>Mutation ops</h3><ol>
<li><code>torch.squeeze()</code>：主要对数据的维度进行压缩，去掉维数为1的的维度，比如是1行或者1列这种，一个1行3列 1x3 的tensor去掉第一个维数为1的维度之后就变成3行。<ul>
<li><code>squeeze(a)</code> 就是将a中所有为1的维度删掉。不为1的维度没有影响;</li>
<li><code>a.squeeze(N)</code> 就是去掉a中指定的维数为1的维度;</li>
<li><code>b=torch.squeeze(a，N)</code> 就是a中去掉指定N维维数为1的维度。</li>
</ul>
</li>
<li><p><code>torch.unsqueeze()</code>: 主要是对数据维度进行扩充。给指定位置加上维数为1的维度，比如原本有个3行的数据 ，在0的位置加了一维就变成一行三列 1x3。</p>
<ul>
<li><code>a.squeeze(N)</code> 就是在a中指定位置N加上一个维数为1的维度;</li>
<li><code>b=torch.squeeze(a，N)</code> 就是在a中指定位置N加上一个维数为1的维度</li>
</ul>
</li>
<li><p><code>torch.transpose(input, dim0, dim1, out=None)</code> : 返回dim0和dim1交换后的tensor</p>
</li>
<li><p><code>torch.t(input, out=None)</code> : 专为2D矩阵的转置，是transpose的便捷函数</p>
</li>
<li><p><code>torch.reshape(input, shape)</code> : 返回size为shape具有相同数值的tensor。Note:shape=(-1,)这种表述，-1表示任意的</p>
</li>
<li><p><code>torch.where(condition,x,y)</code> : 根据condition的值来相应x,y的值，true返回x的值，false返回y的值，形成新的tensor</p>
</li>
<li><p><code>torch.unbind(tensor, dim=0)</code> : 返回tuple 解除指定的dim的绑定,相当于按指定dim拆分</p>
</li>
<li><p><code>torch.nonzero(input, out=None)</code> : 返回非零值的索引， 每一行都是一个非零值的索引值</p>
</li>
</ol>
<h2 id="点对点操作"><a href="#点对点操作" class="headerlink" title="点对点操作"></a>点对点操作</h2><h3 id="基本运算"><a href="#基本运算" class="headerlink" title="基本运算"></a>基本运算</h3><ol>
<li><p><code>torch.add(input, value, out=None)</code></p>
<ul>
<li>add(input, value=1, other, out=None)</li>
<li>addcdiv(tensor, value=1, tensor1, tensor2, out=None)</li>
<li>addcmul(tensor, value=1, tensor1, tensor2, out=None)</li>
</ul>
</li>
<li><p><code>torch.div(input, value, out=None)</code></p>
<ul>
<li>div(input, other, out=None)</li>
</ul>
</li>
<li><p><code>torch.mul(input, value, out=None)</code></p>
<ul>
<li>mul(input, other, out=None)</li>
</ul>
</li>
</ol>
<h3 id="三角函数"><a href="#三角函数" class="headerlink" title="三角函数"></a>三角函数</h3><ol>
<li><p><code>torch.abs(input, out=None)</code> : 绝对值</p>
</li>
<li><p><code>torch.acos(input, out=None)</code></p>
</li>
<li><p><code>torch.asin(input, out=None)</code></p>
</li>
<li><p><code>torch.atan(input, out=None)</code></p>
</li>
<li><p><code>torch.atan2(input, inpu2, out=None)</code></p>
</li>
<li><p><code>torch.cos(input, out=None)</code> : 余弦</p>
</li>
<li><p><code>torch.cosh(input, out=None)</code></p>
</li>
<li><p><code>torch.sin(input, out=None)</code> : 正弦</p>
</li>
<li><p><code>torch.sinh(input, out=None)</code></p>
</li>
<li><p><code>torch.tan(input, out=None)</code> : 正切</p>
</li>
<li><p><code>torch.tanh(input, out=None)</code> : tanh激活函数</p>
</li>
</ol>
<h3 id="对数运算"><a href="#对数运算" class="headerlink" title="对数运算"></a>对数运算</h3><ol>
<li><p><code>torch.log(input, out=None)</code> : $y_i=ln(x_i)$</p>
</li>
<li><p><code>torch.log1p(input, out=None)</code> : $y_i=ln(x_i+1)$</p>
</li>
<li><p><code>torch.log2(input, out=None)</code> : $y_i=log_2(x_i)$</p>
</li>
<li><p><code>torch.log10(input,out=None)</code> : $y_i=log_{10}(x_i)$</p>
</li>
</ol>
<h3 id="指数运算"><a href="#指数运算" class="headerlink" title="指数运算"></a>指数运算</h3><ol>
<li><p><code>torch.exp(tensor, out=None)</code> : $y_i=e^{x_i}$</p>
</li>
<li><p><code>torch.expm1(tensor, out=None)</code> : $y_i=e^{x_i} -1$</p>
</li>
</ol>
<h3 id="幂函数"><a href="#幂函数" class="headerlink" title="幂函数"></a>幂函数</h3><p><code>torch.pow(input, exponent, out=None)</code> : $y_i=input^{exponent}$</p>
<h3 id="截断函数"><a href="#截断函数" class="headerlink" title="截断函数"></a>截断函数</h3><ol>
<li><code>torch.ceil(input, out=None)</code> : 返回向正方向取得最小整数</li>
<li><p><code>torch.floor(input, out=None)</code> : 返回向负方向取得最大整数</p>
</li>
<li><p><code>torch.round(input, out=None)</code> : 返回相邻最近的整数，四舍五入</p>
</li>
<li><p><code>torch.trunc(input, out=None)</code> : 返回整数部分数值</p>
</li>
<li><p><code>torch.frac(tensor, out=None)</code> : 返回小数部分数值</p>
</li>
<li><p><code>torch.fmod(input, divisor, out=None)</code> : 返回input/divisor的余数</p>
</li>
<li><code>torch.remainder(input, divisor, out=None)</code> : 同上</li>
</ol>
<h3 id="其他运算"><a href="#其他运算" class="headerlink" title="其他运算"></a>其他运算</h3><ol>
<li><p><code>torch.erf(tensor, out=None)</code></p>
</li>
<li><p><code>torch.erfinv(tensor, out=None)</code></p>
</li>
<li><p><code>torch.sigmoid(input, out=None)</code> : $y = 1/(1+e^{-input})$</p>
</li>
<li><p><code>torch.clamp(input, min, max out=None)</code> : 返回 input<min,则返回min, input>max,则返回max,其余返回input</min,则返回min,></p>
</li>
<li><p><code>torch.neg(input, out=None)</code> : $out_i=-1*(input)$</p>
</li>
<li><p><code>torch.reciprocal(input, out=None)</code>  : $out_i= 1/input_i$</p>
</li>
<li><p><code>torch.sqrt(input, out=None)</code>  : $out_i=sqrt(input_i)$</p>
</li>
<li><p><code>torch.rsqrt(input, out=None)</code> : $out_i=1/(sqrt(input_i))$</p>
</li>
<li><p><code>torch.sign(input, out=None)</code>  : $out_i=sin(input_i)$  大于0为1，小于0为-1</p>
</li>
<li><p><code>torch.lerp(start, end, weight, out=None)</code></p>
</li>
</ol>
<h2 id="降维操作"><a href="#降维操作" class="headerlink" title="降维操作"></a>降维操作</h2><ol>
<li><code>torch.argmax(input, dim=None, keepdim=False)</code> : 返回最大值排序的索引值</li>
<li><p><code>torch.argmin(input, dim=None, keepdim=False)</code> : 返回最小值排序的索引值</p>
</li>
<li><p><code>torch.cumprod(input, dim, out=None)</code> : $y_i=x_1 <em> x_2 </em> x_3 <em>…</em> x_i$</p>
</li>
<li><p><code>torch.cumsum(input, dim, out=None)</code> : $y_i=x_1 + x_2 + … + x_i$</p>
</li>
<li><p><code>torch.dist(input, out, p=2)</code> : 返回input和out的p式距离</p>
</li>
<li><code>torch.mean()</code> : 返回平均值</li>
<li><code>torch.sum()</code> : 返回总和</li>
<li><code>torch.median(input)</code> : 返回中间值</li>
<li><code>torch.mode(input)</code> : 返回众数值</li>
<li><code>torch.unique(input, sorted=False)</code> : 返回1-D的唯一的tensor,每个数值返回一次.</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">output = torch.unique(torch.tensor([<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=torch.long))</span><br><span class="line">output  <span class="comment"># tensor([ 2,  3,  1])</span></span><br></pre></td></tr></table></figure>
<ol>
<li><code>torch.std()</code> : 返回标准差</li>
<li><code>torch.var()</code> : 返回方差</li>
<li><code>torch.norm(input, p=2)</code> : 返回p-norm的范式</li>
<li><code>torch.prod(input, dim, keepdim=False)</code> : 返回指定维度每一行的乘积</li>
</ol>
<h2 id="对比操作"><a href="#对比操作" class="headerlink" title="对比操作"></a>对比操作</h2><ol>
<li><code>torch.eq(input, other, out=None)</code> : 按成员进行等式操作，相同返回1</li>
<li><code>torch.equal(tensor1, tensor2)</code> : 如果tensor1和tensor2有相同的size和elements，则为true</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">torch.eq(torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]), torch.tensor([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">4</span>, <span class="number">4</span>]]))</span><br><span class="line"><span class="comment"># tensor([[ 1,  0],</span></span><br><span class="line"><span class="comment">#         [ 0,  1]], dtype=torch.uint8)</span></span><br><span class="line"></span><br><span class="line">torch.equal(torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]), torch.tensor([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">4</span>, <span class="number">4</span>]]))</span><br><span class="line"><span class="comment"># False</span></span><br></pre></td></tr></table></figure>
<ol>
<li><code>torch.ge(input, other, out=None)</code> : input&gt;= other</li>
<li><code>torch.gt(input, other, out=None)</code> : input&gt;other</li>
<li><code>torch.le(input, other, out=None)</code> : input=&lt;other</li>
<li><code>torch.lt(input, other, out=None)</code> : input&lt;other</li>
<li><code>torch.ne(input, other, out=None)</code> : input != other 不等于</li>
<li><code>torch.max()</code> : 返回最大值</li>
<li><code>torch.min()</code> : 返回最小值</li>
<li><code>torch.isnan(tensor)</code> : 判断是否为’nan’</li>
<li><code>torch.sort(input, dim=None, descending=False, out=None)</code> : 对目标input进行排序</li>
<li><code>torch.topk(input, k, dim=None, largest=True, sorted=True, out=None)</code> : 沿着指定维度返回最大k个数值及其索引值</li>
<li><code>torch.kthvalue(input, k, dim=None, deepdim=False, out=None)</code> : 沿着指定维度返回最小k个数值及其索引值</li>
</ol>
<h2 id="频谱操作"><a href="#频谱操作" class="headerlink" title="频谱操作"></a>频谱操作</h2><ol>
<li><code>torch.fft(input, signal_ndim, normalized=False)</code> : 傅里叶变换</li>
<li><code>torch.ifft(input, signal_ndim, normalized=False)</code></li>
<li><code>torch.rfft(input, signal_ndim, normalized=False, onesided=True)</code></li>
<li><code>torch.irfft(input, signal_ndim, normalized=False, onesided=True)</code></li>
<li><code>torch.stft(signa, frame_length, hop, …)</code></li>
</ol>
<h2 id="其他操作"><a href="#其他操作" class="headerlink" title="其他操作"></a>其他操作</h2><ol>
<li><p><code>torch.cross(input, other, dim=-1, out=None)</code> : 叉乘(外积)</p>
</li>
<li><p><code>torch.dot(tensor1, tensor2)</code> : 返回tensor1和tensor2的点乘</p>
</li>
<li><p><code>torch.mm(mat1, mat2, out=None)</code> : 返回矩阵mat1和mat2的乘积</p>
</li>
<li><p><code>torch.eig(a, eigenvectors=False, out=None)</code> : 返回矩阵a的特征值/特征向量 </p>
</li>
<li><p><code>torch.det(A)</code> : 返回矩阵A的行列式</p>
</li>
<li><p><code>torch.trace(input)</code> : 返回2-D 矩阵的迹(对角元素求和)</p>
</li>
<li><p><code>torch.diag(input, diagonal=0, out=None)</code> : 返回以input为对角线元素的2D矩阵</p>
</li>
<li><p><code>torch.histc(input, bins=100, min=0, max=0, out=None)</code> : 计算input的直方图</p>
</li>
<li><p><code>torch.tril(input, diagonal=0, out=None)</code> : 返回矩阵的下三角矩阵，其他为0</p>
</li>
<li><p><code>torch.triu(input, diagonal=0, out=None)</code> : 返回矩阵的上三角矩阵，其他为0</p>
</li>
</ol>
<h1 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h1><h2 id="In-place-operation-amp-Out"><a href="#In-place-operation-amp-Out" class="headerlink" title="In-place operation &amp; Out"></a>In-place operation &amp; Out</h2><p><code>in-place operation</code>在PyTorch中是指改变一个tensor的值的时候，不经过复制操作，而是直接在原来的内存上改变它的值。可以把它成为原地操作符。</p>
<p>在pytorch中经常加后缀”_”来代表原地in-place operation，比如说 add_() 或者 scatter()。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = torch.zeros(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">y = torch.ones(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">x.add_(y)           <span class="comment"># Same as x = x + y</span></span><br><span class="line"></span><br><span class="line">x = torch.zeros(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">y = torch.ones(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">torch.add(x, y)     <span class="comment"># Same as z = x + y</span></span><br></pre></td></tr></table></figure>
<h2 id="Tensor-meta-data"><a href="#Tensor-meta-data" class="headerlink" title="Tensor meta-data"></a>Tensor meta-data</h2><ol>
<li><p><code>torch.is_tensor()</code>  : 如果是pytorch的tensor类型返回true</p>
</li>
<li><p><code>torch.is_storage()</code> : 如果是pytorch的storage类型返回ture</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x.size()  <span class="comment"># Size of tensor 获取tensor形状</span></span><br><span class="line"></span><br><span class="line">torch.numel(x)  <span class="comment"># Number of elements in x tensor元素个数</span></span><br><span class="line"></span><br><span class="line">a = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">a[<span class="number">0</span>]  <span class="comment"># 直接取索引返回的是tensor数据</span></span><br><span class="line">a[<span class="number">0</span>].item()  <span class="comment"># 获取python number</span></span><br></pre></td></tr></table></figure>
<h2 id="常用内置函数"><a href="#常用内置函数" class="headerlink" title="常用内置函数"></a>常用内置函数</h2><p>通过一些内置函数，可以实现对tensor的精度, 类型，print打印参数等进行设置.</p>
<ol>
<li><p><code>torch.set_default_dtype(d)</code> : 对torch.tensor() 设置默认的浮点类型</p>
</li>
<li><p><code>torch.set_default_tensor_type()</code> : 同上，对torch.tensor()设置默认的tensor类型</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1.2</span>, <span class="number">3</span>]).dtype       <span class="comment"># initial default for floating point is torch.float32</span></span><br><span class="line"><span class="comment"># torch.float32</span></span><br><span class="line">torch.set_default_dtype(torch.float64)</span><br><span class="line">torch.tensor([<span class="number">1.2</span>, <span class="number">3</span>]).dtype           <span class="comment"># a new floating point tensor</span></span><br><span class="line"><span class="comment"># torch.float64</span></span><br><span class="line">torch.set_default_tensor_type(torch.DoubleTensor)</span><br><span class="line">torch.tensor([<span class="number">1.2</span>, <span class="number">3</span>]).dtype    <span class="comment"># a new floating point tensor</span></span><br><span class="line"><span class="comment"># torch.float64</span></span><br></pre></td></tr></table></figure>
<ol>
<li><p><code>torch.get_default_dtype()</code> #获得当前默认的浮点类型torch.dtype</p>
</li>
<li><p><code>torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None)</code> : 设置printing的打印参数</p>
</li>
</ol>
<h1 id="A-summary-of-available-operations"><a href="#A-summary-of-available-operations" class="headerlink" title="A summary of available operations"></a>A summary of available operations</h1><h2 id="Tensors"><a href="#Tensors" class="headerlink" title="Tensors"></a>Tensors</h2><ul>
<li>autofunction: <code>is_tensor</code></li>
<li>autofunction: <code>is_storage</code></li>
<li>autofunction: <code>set_default_tensor_type</code></li>
<li>autofunction: <code>numel</code></li>
<li>autofunction: <code>set_printoptions</code></li>
</ul>
<h2 id="Serialization"><a href="#Serialization" class="headerlink" title="Serialization"></a>Serialization</h2><ul>
<li>autofunction: <code>save</code>         - Saves an object to a disk file</li>
<li>autofunction: <code>load</code>          - Loads an object saved with torch.save() from a file</li>
</ul>
<h2 id="Parallelism"><a href="#Parallelism" class="headerlink" title="Parallelism"></a>Parallelism</h2><ul>
<li>autofunction: <code>get_num_threads</code> - Gets the number of OpenMP threads used for parallelizing CPU operations</li>
<li>autofunction: <code>set_num_threads</code></li>
</ul>
<h2 id="Spectral-Ops"><a href="#Spectral-Ops" class="headerlink" title="Spectral Ops"></a>Spectral Ops</h2><ul>
<li>autofunction: <code>stft</code>         - Short-time Fourier transform </li>
<li>autofunction: <code>hann_window</code>   - Hann window function</li>
<li>autofunction: <code>hamming_window</code>  - Hamming window function</li>
<li>autofunction: <code>bartlett_window</code> - Bartlett window function</li>
</ul>
<h2 id="BLAS-and-LAPACK-Operations"><a href="#BLAS-and-LAPACK-Operations" class="headerlink" title="BLAS and LAPACK Operations"></a>BLAS and LAPACK Operations</h2><ul>
<li>autofunction: <code>addbmm</code>          - Batch add and mulitply matrices nxp + b×n×m X b×m×p -&gt; bxnxp</li>
<li>autofunction: <code>addmm</code>           - Add and mulitply matrices nxp + n×m X m×p -&gt; nxp</li>
<li>autofunction: <code>addmv</code>           - Add and matrix, vector multipy n + nxm X m -&gt; n</li>
<li>autofunction: <code>addr</code>            - Outer product of vectors</li>
<li>autofunction: <code>baddbmm</code>         - Batch add and mulitply matrices</li>
<li>autofunction: <code>bmm</code>             - Batch mulitply matrices b×n×m X b×m×p -&gt; b×n×p</li>
<li>autofunction: <code>btrifact</code>        - LU factorization</li>
<li>autofunction: <code>btrifact_with_info</code></li>
<li>autofunction: <code>btrisolve</code></li>
<li>autofunction: <code>btriunpack</code></li>
<li>autofunction: <code>dot</code>             - Dot product of 2 tensors</li>
<li>autofunction: <code>eig</code>             - Eigenvalues and eigenvectors ofsquare matrix</li>
<li>autofunction: <code>gels</code>            - Solution for least square or p-norm(AX - B)</li>
<li>autofunction: <code>geqrf</code></li>
<li>autofunction: <code>ger</code>             - Outer product of 2 vectors</li>
<li>autofunction: <code>gesv</code>            - Solve linear equations</li>
<li>autofunction: <code>inverse</code>         - Inverse of square matrix</li>
<li>autofunction: <code>det</code>            - Determinant of a 2D square Variable</li>
<li>autofunction: <code>matmul</code>          - Matrix product of tensors</li>
<li>autofunction: <code>mm</code>                - Matrix multiplication</li>
<li>autofunction: <code>mv</code>              - Matrix vector product</li>
<li>autofunction: <code>orgqr</code>           - Orthogal matrix Q </li>
<li>autofunction: <code>ormqr</code>           - Multiplies matrix by the orthogonal Q matrix</li>
<li>autofunction: <code>potrf</code>           - Cholesky decomposition</li>
<li>autofunction: <code>potri</code>           - Inverse of a positive semidefinite matrix with Cholesky</li>
<li>autofunction: <code>potrs</code>           - Solve linear equation with positive semidefinite</li>
<li>autofunction: <code>pstrf</code>           - Cholesky decomposition of a positive semidefinite matrix</li>
<li>autofunction: <code>qr</code>              - QR decomposition</li>
<li>autofunction: <code>svd</code>             - SVD decomposition</li>
<li>autofunction: <code>symeig</code>          - Eigenvalues and eigenvectors</li>
<li>autofunction: <code>trtrs</code>           - Solves a system of equations with a triangular coefficient</li>
</ul>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
        <div class="post-widgets">
        

        

        
          
          <div id="needsharebutton-postbottom">
            <span class="btn">
              <i class="fa fa-share-alt" aria-hidden="true"></i>
            </span>
          </div>
        
        </div>
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/07/29/Autograd/" rel="next" title="Autograd">
                <i class="fa fa-chevron-left"></i> Autograd
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/07/31/PyTorch-Data-Generator/" rel="prev" title="PyTorch Data Generator">
                PyTorch Data Generator <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      
        <div id="gitment-container"></div>
      
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/messi.jpg" alt="MARS">
            
              <p class="site-author-name" itemprop="name">MARS</p>
              <p class="site-description motion-element" itemprop="description">自由是一种信仰</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/JyZhang10Mars" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="jyzhang.mars@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Tensors-attributes"><span class="nav-number">1.</span> <span class="nav-text">Tensors attributes</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Torch-dtype"><span class="nav-number">1.1.</span> <span class="nav-text">Torch.dtype</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Torch-device"><span class="nav-number">1.2.</span> <span class="nav-text">Torch.device</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Torch-layout"><span class="nav-number">1.3.</span> <span class="nav-text">Torch.layout</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Create-a-Tensor"><span class="nav-number">2.</span> <span class="nav-text">Create a Tensor</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#直接创建"><span class="nav-number">2.1.</span> <span class="nav-text">直接创建</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#从Numpy-创建"><span class="nav-number">2.2.</span> <span class="nav-text">从Numpy 创建</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#创建特定的Tensor"><span class="nav-number">2.3.</span> <span class="nav-text">创建特定的Tensor</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#根据数值要求"><span class="nav-number">2.3.1.</span> <span class="nav-text">根据数值要求</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#根据矩阵要求"><span class="nav-number">2.3.2.</span> <span class="nav-text">根据矩阵要求</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#随机采样生成"><span class="nav-number">2.4.</span> <span class="nav-text">随机采样生成</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Tensor-Operations"><span class="nav-number">3.</span> <span class="nav-text">Tensor Operations</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#基本操作"><span class="nav-number">3.1.</span> <span class="nav-text">基本操作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#索引-amp-掩码"><span class="nav-number">3.1.1.</span> <span class="nav-text">索引 &amp; 掩码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#连接-amp-堆栈"><span class="nav-number">3.1.2.</span> <span class="nav-text">连接 &amp; 堆栈</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#分割-amp-分块"><span class="nav-number">3.1.3.</span> <span class="nav-text">分割 &amp; 分块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Mutation-ops"><span class="nav-number">3.1.4.</span> <span class="nav-text">Mutation ops</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#点对点操作"><span class="nav-number">3.2.</span> <span class="nav-text">点对点操作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#基本运算"><span class="nav-number">3.2.1.</span> <span class="nav-text">基本运算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#三角函数"><span class="nav-number">3.2.2.</span> <span class="nav-text">三角函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#对数运算"><span class="nav-number">3.2.3.</span> <span class="nav-text">对数运算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#指数运算"><span class="nav-number">3.2.4.</span> <span class="nav-text">指数运算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#幂函数"><span class="nav-number">3.2.5.</span> <span class="nav-text">幂函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#截断函数"><span class="nav-number">3.2.6.</span> <span class="nav-text">截断函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#其他运算"><span class="nav-number">3.2.7.</span> <span class="nav-text">其他运算</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#降维操作"><span class="nav-number">3.3.</span> <span class="nav-text">降维操作</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#对比操作"><span class="nav-number">3.4.</span> <span class="nav-text">对比操作</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#频谱操作"><span class="nav-number">3.5.</span> <span class="nav-text">频谱操作</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#其他操作"><span class="nav-number">3.6.</span> <span class="nav-text">其他操作</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Tips"><span class="nav-number">4.</span> <span class="nav-text">Tips</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#In-place-operation-amp-Out"><span class="nav-number">4.1.</span> <span class="nav-text">In-place operation &amp; Out</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Tensor-meta-data"><span class="nav-number">4.2.</span> <span class="nav-text">Tensor meta-data</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#常用内置函数"><span class="nav-number">4.3.</span> <span class="nav-text">常用内置函数</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#A-summary-of-available-operations"><span class="nav-number">5.</span> <span class="nav-text">A summary of available operations</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Tensors"><span class="nav-number">5.1.</span> <span class="nav-text">Tensors</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Serialization"><span class="nav-number">5.2.</span> <span class="nav-text">Serialization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Parallelism"><span class="nav-number">5.3.</span> <span class="nav-text">Parallelism</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spectral-Ops"><span class="nav-number">5.4.</span> <span class="nav-text">Spectral Ops</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#BLAS-and-LAPACK-Operations"><span class="nav-number">5.5.</span> <span class="nav-text">BLAS and LAPACK Operations</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">MARS</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    
      <div id="needsharebutton-float">
        <span class="btn">
          <i class="fa fa-share-alt" aria-hidden="true"></i>
        </span>
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  
  
  <link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css">

  
  
  <script src="/lib/needsharebutton/needsharebutton.js"></script>

  <script>
    
      pbOptions = {};
      
          pbOptions.iconStyle = "box";
      
          pbOptions.boxForm = "horizontal";
      
          pbOptions.position = "bottomCenter";
      
          pbOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
      
      new needShareButton('#needsharebutton-postbottom', pbOptions);
    
    
      flOptions = {};
      
          flOptions.iconStyle = "box";
      
          flOptions.boxForm = "horizontal";
      
          flOptions.position = "middleRight";
      
          flOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
      
      new needShareButton('#needsharebutton-float', flOptions);
    
  </script>

  

  
  


  

  

</body>
</html>
